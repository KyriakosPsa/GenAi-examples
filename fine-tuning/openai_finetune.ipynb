{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_data = pd.read_parquet(\"train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create function to prepare the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'You are a helpful SQL coding assistant, you are to '\n",
      "                          'form SQL queries based on questions asked',\n",
      "               'role': 'system'},\n",
      "              {'content': 'How many heads of the departments are older than 56 '\n",
      "                          '?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'SELECT count(*) FROM head WHERE age  >  56',\n",
      "               'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a helpful SQL coding assistant, you are to form SQL queries based on questions asked\"\n",
    "\n",
    "\n",
    "def prepare_example_conversation(row):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": row[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": row[\"query\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "pprint(prepare_example_conversation(sql_data.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 6)\n",
      "(5600, 6)\n",
      "(1400, 6)\n"
     ]
    }
   ],
   "source": [
    "print(sql_data.shape)\n",
    "train_data = sql_data.iloc[0:5600]\n",
    "print(train_data.shape)\n",
    "validation_data = sql_data.iloc[5600:]\n",
    "print(validation_data.shape)\n",
    "\n",
    "train_data = train_data.apply(prepare_example_conversation, axis=1).to_list()\n",
    "validation_data = validation_data.apply(prepare_example_conversation, axis=1).to_list()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write to json files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data_list: list, filename: str) -> None:\n",
    "    with open(filename, \"w\") as out:\n",
    "        for ddict in data_list:\n",
    "            jout = json.dumps(ddict) + \"\\n\"\n",
    "            out.write(jout)\n",
    "\n",
    "\n",
    "training_file_name = \"training.jsonl\"\n",
    "write_jsonl(train_data, training_file_name)\n",
    "\n",
    "validation_file_name = \"validation.jsonl\"\n",
    "write_jsonl(validation_data, validation_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-MuCMdHXPKfN7ayZPBYpTaY\n",
      "Validation file ID: file-Qh8Y1jMvNzkoXwsNG7L2uR\n"
     ]
    }
   ],
   "source": [
    "def upload_file(file_name: str, purpose: str) -> str:\n",
    "    with open(file_name, \"rb\") as file_fd:\n",
    "        response = client.files.create(file=file_fd, purpose=purpose)\n",
    "    return response.id\n",
    "\n",
    "\n",
    "training_file_id = upload_file(training_file_name, \"fine-tune\")\n",
    "validation_file_id = upload_file(validation_file_name, \"fine-tune\")\n",
    "\n",
    "print(\"Training file ID:\", training_file_id)\n",
    "print(\"Validation file ID:\", validation_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Fq9LSZ9miWheThL6Fr9mie8f\n",
      "Status: validating_files\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=MODEL,\n",
    "    suffix=\"recipe-ner\",\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-Fq9LSZ9miWheThL6Fr9mie8f\n",
      "Status: validating_files\n",
      "Trained Tokens: None\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(\"Trained Tokens:\", response.trained_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-HarNc5RHL68iqgIfH46rU5Ur\n",
      "Validating training file: file-HvUwgMUoNQxByeWgySnKpE and validation file: file-WopdyeHHBmdX31HLpHe3tJ\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.list_events(job_id)\n",
    "\n",
    "events = response.data\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event.message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
